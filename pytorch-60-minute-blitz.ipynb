{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: _What_ is PyTorch?\n",
    "---\n",
    "[Source](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py) for this tutorial\n",
    "\n",
    "<br></br>\n",
    "<dl>\n",
    "    <dt>PyTorch</dt>\n",
    "    <dd>is a Python-based computing package</dd>\n",
    "</dl>\n",
    "\n",
    "- A programming language that uses the power of GPU's to speed up calculations.\n",
    "> I don't have an <font color=green>NVIDIA GPU</font> at the moment but I'll press on.\n",
    ">\n",
    "> I'll rent one here -> [NVIDIA GPU in the clouds above](https://cloud.google.com/)\n",
    "- It's flexible and F.A.S.T.\n",
    "    > `Python` + \"...dang it's so fast it lit a <font color=red>_Torch_</font>\" == __`PyTorch`__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "### Tensors\n",
    "\n",
    "As I discovered in the first tutorial, `PyTorch` is similar to` NumPy`. Again, `PyTorch` uses __GPU's__, which makes it faster than `NumPy` for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"An __uninitialized matrix__ is declared, but _does not_ contain __definite known values__ before it is used. When an uninitialized matrix is created, whatever values were in the allocated memory at the time will appear as the initial values.\"\n",
    "\n",
    "### Creating an empty Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6816e-44, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Creating an 'uninitialized matrix' with `tensor.empty()`\n",
    "# So does it not have values?\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interesting, the \"values\" (0's) that were placed in the empty tensor matrix\n",
    "# changed to actual values once operated on...\n",
    "x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Random Martrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.randn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0916, -0.3037,  0.6995,  0.7574,  0.4234, -0.3557, -0.8298],\n",
      "        [-0.2322,  0.6200,  0.3670,  1.0628,  0.1133, -1.2792, -0.0153],\n",
      "        [ 1.6407, -1.1008, -2.5461, -1.7217,  0.8697,  1.6432, -0.5604],\n",
      "        [-0.7609, -0.9253, -1.0508, -0.3378, -0.1354, -0.0995, -0.1097],\n",
      "        [-0.9636,  0.3893,  0.6245, -0.0587,  1.9228, -1.3758,  1.3084]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 7, dtype=torch.float64)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Matrix of Zeros and Ones\n",
    "\n",
    "https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5, 5, dtype=torch.long)  # 64-bit integer (signed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5, 5, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(5, 5, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(5, 5, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.+0.j, 1.+0.j, 1.+0.j],\n",
       "        [1.+0.j, 1.+0.j, 1.+0.j],\n",
       "        [1.+0.j, 1.+0.j, 1.+0.j]], dtype=torch.complex128)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 3, dtype=torch.cdouble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "PyTorch tensors are unable to infer True as 1 and False as 0's\n",
    "``` python\n",
    "torch.ones(5, 5, dtype=torch.bool).mean()\n",
    "```\n",
    "<font color=red>RuntimeError</font>: Can only calculate the mean of floating types. Got Bool instead.\n",
    "\n",
    "\n",
    "### Create a Tensor from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tensor using NumPy to generate data\n",
    "x = torch.tensor(np.random.randint(1, 11, size=(10, 10)))\n",
    "\n",
    "# Create a Tensor using PyTorch built in method `.randint()`\n",
    "y = torch.randint(1, 11, size=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  5, 10,  4,  2, 10,  9, 10,  9,  3],\n",
       "        [ 5,  5,  6,  9,  4,  2,  3,  5, 10, 10],\n",
       "        [ 3,  8,  5,  1,  4, 10,  1,  6,  3,  6],\n",
       "        [ 3,  3,  2,  6,  9, 10,  3,  9,  5, 10],\n",
       "        [ 4,  6,  6,  8,  9,  4,  1,  4,  6,  1],\n",
       "        [ 5,  2,  3,  8,  2,  7,  5,  1, 10,  9],\n",
       "        [ 8,  8, 10,  2,  7,  5,  4,  8,  9,  6],\n",
       "        [ 2,  8,  1,  4,  7,  7, 10,  4,  4,  6],\n",
       "        [ 5,  9,  2,  3,  6,  7,  6,  7,  3,  4],\n",
       "        [ 9,  3,  5,  5,  4,  9, 10,  7, 10,  3]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.dtype)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  9,  5,  8,  8, 10,  6,  3,  9],\n",
       "        [ 6, 10,  5,  8,  6,  2,  7,  8,  3,  1],\n",
       "        [ 8, 10,  6,  5,  2,  4,  9,  2,  6,  1],\n",
       "        [ 6,  3,  3, 10,  6, 10,  8,  7,  6,  7],\n",
       "        [ 8,  6,  4,  8,  8,  2,  6,  8,  5,  8],\n",
       "        [ 4,  4,  2,  1,  5,  5, 10,  7,  3,  1],\n",
       "        [ 7,  3,  2,  5, 10,  5,  2,  5,  6,  4],\n",
       "        [10,  6,  3,  4,  3,  1,  6,  3,  4,  4],\n",
       "        [ 4,  5, 10,  2,  4,  9,  7,  8, 10, 10],\n",
       "        [ 7,  4,  5,  9, 10,  8, 10,  2,  1,  8]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y.dtype)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Tensor based on an existing Tensor\n",
    "#### Creating Tensors with new dimensions, dtypes, and requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .new_*() methods, we can create new Tensors!\n",
    "# If needed, we can also change the dtype\n",
    "\n",
    "new_x1 = x.new_ones(3, 3)\n",
    "new_x2 = x.new_ones(5, 5, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(new_x1.dtype)\n",
    "new_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(new_x2.dtype)\n",
    "new_x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly created Tensors do not have `requires_grad=True` because:\n",
    "1. The original Tensor it was created from did not have `requires_grad=True`\n",
    "2. When creating the Tensor from an exisiting Tensor, __WE__ did not specifiy `requires_grad=True`.\n",
    "\n",
    ">__REMINDER__! `requires_grad=True` can only be set on Tensors of `dtype=float`!\n",
    "> Example:\n",
    "> ``` python\n",
    "> # new_x1.dtype >>> torch.int64\n",
    "new_x1.requires_grad_()\n",
    "> ```\n",
    "> If you try to set `requires_grad=True` on a Tensor of dtype `torch.int64` you'll get the following error.\n",
    ">\n",
    "> <font color=red>RuntimeError</font>:\n",
    ">```python \n",
    "Only Tensors of float point dtype can require gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(new_x1.requires_grad)\n",
    "print(new_x2.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# print(new_x1.requires_grad_())\n",
    "\n",
    "# Now our new tensor has memory.\n",
    "print(new_x2.requires_grad_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors from Tensors using the SAME dimensions, different dtypes, requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x was initialized with the data type `int`\n",
    "x_new_full = x.new_full(x.shape, 10, dtype=float).requires_grad_()\n",
    "y_new_full = x.new_full(x.size(), 10, dtype=float).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.size())\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "torch.Size([10, 10])\n",
      "torch.float64\n",
      "\n",
      "y\n",
      "torch.Size([10, 10])\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "print('x')\n",
    "print(x_new_full.shape)\n",
    "print(x_new_full.dtype)\n",
    "\n",
    "print('\\ny')\n",
    "print(y_new_full.shape)\n",
    "print(y_new_full.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8623, 1.4138, 0.5886],\n",
      "        [1.0883, 1.5882, 1.2020],\n",
      "        [0.6420, 0.5145, 1.4439],\n",
      "        [0.5153, 1.1107, 1.2614],\n",
      "        [1.2066, 0.7039, 1.4612]])\n"
     ]
    }
   ],
   "source": [
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8623, 1.4138, 0.5886],\n",
      "        [1.0883, 1.5882, 1.2020],\n",
      "        [0.6420, 0.5145, 1.4439],\n",
      "        [0.5153, 1.1107, 1.2614],\n",
      "        [1.2066, 0.7039, 1.4612]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8623, 1.4138, 0.5886],\n",
      "        [1.0883, 1.5882, 1.2020],\n",
      "        [0.6420, 0.5145, 1.4439],\n",
      "        [0.5153, 1.1107, 1.2614],\n",
      "        [1.2066, 0.7039, 1.4612]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8623, 1.4138, 0.5886],\n",
       "        [1.0883, 1.5882, 1.2020],\n",
       "        [0.6420, 0.5145, 1.4439],\n",
       "        [0.5153, 1.1107, 1.2614],\n",
       "        [1.2066, 0.7039, 1.4612]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8623, 1.4138, 0.5886],\n",
       "        [1.0883, 1.5882, 1.2020],\n",
       "        [0.6420, 0.5145, 1.4439],\n",
       "        [0.5153, 1.1107, 1.2614],\n",
       "        [1.2066, 0.7039, 1.4612]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.shape, z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(1, 11, size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10,  6,  2],\n",
      "        [ 9,  4,  4],\n",
      "        [ 8, 10,  1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x)\n",
    "x[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Bridge\n",
    "---\n",
    "Converting a Torch Tensor to a NumPy array and vice a versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)\n",
    "print(b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting a NumPy Array to Torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Autograd: Automatic Differentiation\n",
    "---\n",
    "[Source](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) of this tutorial\n",
    "\n",
    "PyTorch functions for auto-gradient implementation:\n",
    "1. `torch.tensor`\n",
    "1. `torch.tensor.requires_grad_()`\n",
    "1. `torch.tensor.backward()`\n",
    "1. `tensor.detach()`\n",
    "1. `tensor.grad_fn`\n",
    "\n",
    "The `autograd` package is the cornerstone to all neural networks in `PyTorch`.\n",
    "- `autograd` provides automatic differentiation fot all operations on all tensors. MEMORY\n",
    "- Backpropagation is defined by how your code is excuted.\n",
    "- Every iteration can be different.\n",
    "\n",
    "## Tensor\n",
    "---\n",
    "__`torch.tensor`__ is the fundamental building block in PyTorch.\n",
    "> If `requires_grad=True`, the tensor begins to \"remember\" all operations on it.\n",
    ">\n",
    "> When you call `.backward()` on a `torch.tensor` object that has the attribute `requires_grad=True`, all gradients are computed automatically.\n",
    "    > - The gradient is stored in the `.grad` attribute.\n",
    "\n",
    "To remove a tensor's \"memory\" use `.detach()` to detach it from the computational history. Detach it from its \"experience\". The tensor will also not be able to \"remember\" any future computations performed on it.\n",
    "\n",
    "To prevent a torch from having memory, wrap the code block with:\n",
    "> `with torch.no_grad():`\n",
    "\n",
    "Note: Useful when evaluating a model because the model may have \"trainable parameters\" with `requires_grad=True` and we __don't need the gradients__.\n",
    "\n",
    "`Function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\n",
    "y = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n",
      "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Notice that when we create a new tensor by adding two tensors that have `requires_grad`=True,\n",
    "# The new tensor `z` has memory of how it was created grad_fn=<AddBackward0>.\n",
    "# It knows it was created by addition!\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tensor by mulitplying two existing tensors and a value/scaler\n",
    "a = z * z * 10\n",
    "a_scalar = a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 40., 160., 360.], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(186.6667, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reminder: Tensors can only contain float dtypes.\n",
    "\n",
    "# Similarly, tensor `a` knows that it was created grad_fn=<MulBackward0> == multiplication!\n",
    "print(a)\n",
    "\n",
    "# We'll use a_scalar in the next section gradients\n",
    "print(a_scalar)  # Interesting, this tensor remember the function used on it: grad_fn=<MeanBackward0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(2, 3)\n",
    "b = ((b*10) / (b-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does tensor `b` have autograd enabled? False\n"
     ]
    }
   ],
   "source": [
    "# Reminder: When you create new tensors, you must explicity set requires_grad=True\n",
    "\n",
    "# This tensor does not have autograd enabled. Tensors created from b will not have autograd enabled.\n",
    "print(f\"Does tensor `b` have autograd enabled? {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3675,  5.2046,  3.6272],\n",
       "        [ 5.6768,  3.4774,  6.8623]], requires_grad=True)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use .requires_grad_() function to add backpropagation to the tensor `b`\n",
    "# Using .requires_grad_(True) modifies the tensor inplace, giving it memory.\n",
    "b.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MulBackward0 object at 0x7fc5d8670d50>\n",
      "None\n",
      "<SumBackward0 object at 0x7fc5d8670d50>\n"
     ]
    }
   ],
   "source": [
    "c = (a * b).sum()\n",
    "print(a.grad_fn)  # created from: z * z * 10\n",
    "print(b.grad_fn)  # A tensor created from scratch will not have memory. Only the ability to memorize.\n",
    "print(c.grad_fn)  # created from: (a * b).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My first backpropagation!\n",
    "a_scalar.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor `x` and tensor `y` are considered leaves, or leaf individually. These two tensors are the origin of `a_scalar`.\n",
    "When backpropagation is executed, the gradient calculations stop at `x` and `y`.\n",
    "\n",
    "`a_scaler` ----Backprop----> `a` ----Backprop----> `z` ----Backprop----> __`x` + `y`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13.3333, 26.6667, 40.0000], dtype=torch.float64)\n",
      "tensor([13.3333, 26.6667, 40.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. Tensor `z` is called a __non-leaf__. Similar to decision tree leafs/pure leafs, this tensor is considered a node. \n",
    "\n",
    "```python\n",
    "print(z.grad)\n",
    "```\n",
    "\n",
    "<div class='alert alert-block alert-danger'>UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
    "This is separate from the ipykernel package so we can avoid doing imports until </div>\n",
    "  \n",
    "### Example of vector-Jacobian product  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 877.5061, -408.6016, -641.1202], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reminder x2: When creating a new tensor, you must explicitly set requires_grad=True\n",
    "# To perform backprop and give the tensor memory.\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:  # x.data returns the values the tensor object with scalar values\n",
    "    y *= 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "x.data.norm()\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop autograd\n",
    "---\n",
    "- `with torch.no_grad():` code block\n",
    "- `.requires_grad_(False)`\n",
    "- `.detach()`\n",
    "\n",
    "#### `with torch.no_grad():` code block\n",
    "- Wrapping a tensor that was created with requires_grad=True will not be able to pass its __memory abilities__ to new tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does tensor `x` have autograd enabled?\n",
      "True\n",
      "\n",
      "What about a new tensor? Would it have autograd enabled it was created from tensor `x`?\n",
      "True\n",
      "\n",
      "Does tensor `x` have auto_grad enabled now that it's in a torch.no_grad() code block?\n",
      "True\n",
      "\n",
      "What about the new tensor? Would it have auto grad enabled now that it's inside a torch.no_grad(): code block?\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Wrapping a tensor that has requires_grad=True inside a code block\n",
    "# That removes the tensors memory as long as it's in the code block\n",
    "\n",
    "# As we're learning, we know that tensors created from tensors that have requires_grad=True will\n",
    "# pass their memory ability to the new tensor\n",
    "print(f\"Does tensor `x` have autograd enabled?\")\n",
    "print(x.requires_grad)\n",
    "print(\"\\nWhat about a new tensor? Would it have autograd enabled it was created from tensor `x`?\")\n",
    "print((x**2).requires_grad)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"\\nDoes tensor `x` have auto_grad enabled now that it's in a torch.no_grad() code block?\")\n",
    "    print(x.requires_grad)\n",
    "    print(\"\\nWhat about the new tensor? Would it have auto grad enabled now that it's inside a torch.no_grad(): code block?\")\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.requires_grad_(False)`\n",
    "- Most explicit way to remove a tensors' memory is by using `tensor_name.requires_grad_(False)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does tensor `x` have auto_grad enabled? True\n",
      "\n",
      "What about now? False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Does tensor `x` have auto_grad enabled? {x.requires_grad}\")\n",
    "x.requires_grad_(False)\n",
    "print()\n",
    "print(f\"What about now? {x.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.detach`\n",
    "- Use `.detach()` to remove autograd but keep contents of tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does tensor `x` have auto_grad enabled? True\n",
      "Does tensor `y` have auto_grad enabled? False\n",
      "\n",
      "Is tensor `x` equal to tensor `y`?\n",
      "tensor(True)\n",
      "tensor([True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# Set tensor `x` with requires_grad=True to walkthrough using detach()\n",
    "x.requires_grad_(True)\n",
    "\n",
    "print(f\"Does tensor `x` have auto_grad enabled? {x.requires_grad}\")\n",
    "\n",
    "# Create a new tensor from `x` that does not have requires_grad=True. Remove its memory.\n",
    "y = x.detach()\n",
    "\n",
    "print(f\"Does tensor `y` have auto_grad enabled? {y.requires_grad}\", end='\\n\\n')\n",
    "\n",
    "# Although `y` does not have autograd enabled, both tensors contain the same values\n",
    "print(\"Is tensor `x` equal to tensor `y`?\")\n",
    "print(x.eq(y).all())\n",
    "\n",
    "# Another proof of equality\n",
    "print(x==y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 Neural Networks\n",
    "\n",
    "Source of [tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html).\n",
    "\n",
    "Neural networks are created with the `torch.nn` package. Neural networks depend on autograd to define models and differentiate them!\n",
    "\n",
    "`nn.Module` contains layers and method `forward()` (feed forward? Yes!) that returns the output.\n",
    "\n",
    "Simple feed-forward networks takes an input, feeds it/pushes it through several layers, and returns an output.\n",
    "\n",
    "Standard Operating Procedure to train neural networks:\n",
    "1. Define the neural network with learnable parameters(or weights).\n",
    "1. Interate over a dataset of inputs.\n",
    "1. Process input through the network. Feed the data forward.\n",
    "1. Compute the loss (how far is the output from being correct/how far off is the output from reality?)\n",
    "1. Propagate gradients back into the networks parameters.\n",
    "1. Update the weights of the NN, using an update rule: weight = weight * (learning_rate * gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating my first neural network! October 7th, 2020 2:59am\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution kernel?\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        # 6 input image channels, 16 output channels, 3x3 square convolution kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        # Calculate weights for each layer\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension?\n",
    "        self.fc2 = nn.Linear(120, 84)  # I vaguely remember doing something like this in Deep Learning\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Feeding the inputs forward through the network\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size of the window is a square, you can only specify a single number.\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "         \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # select all dimensions except for the batch dimension. 16?\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 10\n",
      "torch.Size([6, 1, 3, 3])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 3, 3])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 576])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Learnable parameters of a model are returned using `net.parameters()`\n",
    "params = list(net.parameters())\n",
    "print(f\"Number of parameters: {len(params)}\")\n",
    "\n",
    "# Each convolution is collapsed on the number of output channels.\n",
    "for i in range(0, 10):\n",
    "    print(params[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. Parameters 0 and 2 hold the weights of conv1 and conv2.\n",
    "Layout of returned parameters at index 0 and 2\n",
    "\n",
    "| Parameter | # Output Channels | # Input Image Channels | Row Length of Convolutional Kernel | Colum Length of Convolution Kernel |\n",
    "|----|----|----|----|----|\n",
    "|params[0]|6|1|3|3|\n",
    "|params[2]|16|6|3|3|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0062,  0.0660,  0.0049, -0.0889, -0.0138, -0.0186,  0.0148,  0.1077,\n",
      "         -0.0276,  0.0883]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(1, 1, 32, 32)\n",
    "out = net(input_tensor)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.rand(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn` only supports __mini-batches__.\n",
    "Naturally, `torch.nn` only supports inputs that are a mini-batch of samples. No single samples. \"All or none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
