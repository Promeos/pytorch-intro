{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: _What_ is PyTorch?\n",
    "---\n",
    "[Source](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py) for this tutorial\n",
    "\n",
    "<br></br>\n",
    "<dl>\n",
    "    <dt>PyTorch</dt>\n",
    "    <dd>is a Python-based computing package</dd>\n",
    "</dl>\n",
    "\n",
    "- A programming language that uses the power of GPU's to speed up calculations.\n",
    "> I don't have an <font color=green>NVIDIA GPU</font> at the moment but I'll press on.\n",
    ">\n",
    "> I'll rent one here -> [NVIDIA GPU in the clouds above](https://cloud.google.com/)\n",
    "- It's flexible and F.A.S.T.\n",
    "    > `Python` + \"...dang it's so fast it lit a <font color=red>_Torch_</font>\" == __`PyTorch`__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "### Tensors\n",
    "\n",
    "As I discovered in the first tutorial, `PyTorch` is similar to` NumPy`. Again, `PyTorch` uses __GPU's__, which makes it faster than `NumPy` for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"An __uninitialized matrix__ is declared, but _does not_ contain __definite known values__ before it is used. When an uninitialized matrix is created, whatever values were in the allocated memory at the time will appear as the initial values.\"\n",
    "\n",
    "### Creating an empty Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -3.6893e+19,  2.7378e+03],\n",
      "        [ 4.6577e-10,  1.8361e+25,  1.4603e-19],\n",
      "        [ 1.6795e+08,  4.7423e+30,  4.7393e+30],\n",
      "        [ 9.5461e-01,  4.4377e+27,  1.7975e+19],\n",
      "        [ 4.6894e+27,  7.9463e+08,  3.2604e-12]])\n"
     ]
    }
   ],
   "source": [
    "# Creating an 'uninitialized matrix' with `tensor.empty()`\n",
    "# So does it not have values?\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -3.6893e+19,  2.7388e+03],\n",
       "        [ 1.0000e+00,  1.8361e+25,  1.0000e+00],\n",
       "        [ 1.6795e+08,  4.7423e+30,  4.7393e+30],\n",
       "        [ 1.9546e+00,  4.4377e+27,  1.7975e+19],\n",
       "        [ 4.6894e+27,  7.9463e+08,  1.0000e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interesting, the \"values\" (0's) that were placed in the empty tensor matrix\n",
    "# changed to actual values once operated on...\n",
    "x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Random Martrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.randn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7779, -0.8111,  1.5805, -0.8017,  0.5302,  0.0360,  1.4072],\n",
      "        [ 1.1951,  0.0636,  0.1992, -0.5046,  0.4514, -0.9847,  0.3981],\n",
      "        [-0.2289, -1.5289,  0.5361,  0.2977, -0.0327, -0.1529,  0.5696],\n",
      "        [-0.5381, -0.3631,  2.4126,  2.4681, -0.0349, -0.9248,  0.0521],\n",
      "        [-0.3913,  1.0341, -1.2679,  0.9111,  0.8743,  0.1460,  1.4379]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 7, dtype=torch.float64)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Matrix of Zeros and Ones\n",
    "\n",
    "https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5, 5, dtype=torch.long)  # 64-bit integer (signed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5, 5, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(5, 5, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(5, 5, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.+0.j, 1.+0.j, 1.+0.j],\n",
       "        [1.+0.j, 1.+0.j, 1.+0.j],\n",
       "        [1.+0.j, 1.+0.j, 1.+0.j]], dtype=torch.complex128)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 3, dtype=torch.cdouble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "PyTorch tensors are unable to infer True as 1 and False as 0's\n",
    "``` python\n",
    "torch.ones(5, 5, dtype=torch.bool).mean()\n",
    "```\n",
    "<font color=red>RuntimeError</font>: Can only calculate the mean of floating types. Got Bool instead.\n",
    "\n",
    "\n",
    "### Create a Tensor from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tensor using NumPy to generate data\n",
    "x = torch.tensor(np.random.randint(1, 11, size=(10, 10)))\n",
    "\n",
    "# Create a Tensor using PyTorch built in method `.randint()`\n",
    "y = torch.randint(1, 11, size=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  9,  8,  7,  7,  9,  8,  8,  9,  4],\n",
       "        [ 1,  9,  7,  4,  6,  6,  8,  4,  6,  8],\n",
       "        [ 9,  6,  7,  7,  8,  6,  8,  9,  5,  9],\n",
       "        [ 1,  3,  8, 10,  2,  4,  9,  2,  4,  2],\n",
       "        [ 9,  1,  7,  6,  9, 10,  1,  5,  3,  1],\n",
       "        [10,  7,  7,  5,  1,  5,  8,  2,  3,  5],\n",
       "        [ 7,  8,  6,  4,  7,  7,  6,  9,  8,  7],\n",
       "        [ 5,  9,  1,  5,  8,  6,  6,  7,  5,  6],\n",
       "        [ 1,  8,  1, 10,  3,  8, 10,  2,  1,  7],\n",
       "        [ 5,  6,  9,  9,  2,  1,  6,  7, 10,  2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.dtype)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  7,  4,  9, 10,  7,  3,  2,  7,  8],\n",
       "        [ 5,  5, 10,  7,  6,  3,  7,  8,  2,  7],\n",
       "        [ 5,  2,  4, 10, 10,  4,  2,  8,  5,  2],\n",
       "        [ 2,  8,  1,  5,  9,  4,  2,  3,  4,  8],\n",
       "        [ 1, 10,  8,  1,  1, 10,  4,  4,  4,  5],\n",
       "        [ 7,  3,  2, 10,  4,  1, 10,  9,  1,  1],\n",
       "        [10,  4,  8,  5,  2,  2,  8,  8,  3,  5],\n",
       "        [ 6,  3,  8,  7, 10,  8,  1,  6,  7, 10],\n",
       "        [ 8,  1,  1,  5, 10,  8,  5,  5,  2,  9],\n",
       "        [ 7,  2,  9,  1, 10,  4,  4,  8,  9, 10]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y.dtype)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Tensor based on an existing Tensor\n",
    "#### Creating Tensors with new dimensions, dtypes, and requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .new_*() methods, we can create new Tensors!\n",
    "# If needed, we can also change the dtype\n",
    "\n",
    "new_x1 = x.new_ones(3, 3)\n",
    "new_x2 = x.new_ones(5, 5, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(new_x1.dtype)\n",
    "new_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(new_x2.dtype)\n",
    "new_x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly created Tensors do not have `requires_grad=True` because:\n",
    "1. The original Tensor it was created from did not have `requires_grad=True`\n",
    "2. When creating the Tensor from an exisiting Tensor, __WE__ did not specifiy `requires_grad=True`.\n",
    "\n",
    ">__REMINDER__! `requires_grad=True` can only be set on Tensors of `dtype=float`!\n",
    "> Example:\n",
    "> ``` python\n",
    "> # new_x1.dtype >>> torch.int64\n",
    "new_x1.requires_grad_()\n",
    "> ```\n",
    "> If you try to set `requires_grad=True` on a Tensor of dtype `torch.int64` you'll get the following error.\n",
    ">\n",
    "> <font color=red>RuntimeError</font>:\n",
    ">```python \n",
    "Only Tensors of float point dtype can require gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(new_x1.requires_grad)\n",
    "print(new_x2.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# print(new_x1.requires_grad_())\n",
    "\n",
    "# Now our new tensor has memory.\n",
    "print(new_x2.requires_grad_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors from Tensors using the SAME dimensions, different dtypes, requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x was initialized with the data type `int`\n",
    "x_new_full = x.new_full(x.shape, 10, dtype=float).requires_grad_()\n",
    "y_new_full = x.new_full(x.size(), 10, dtype=float).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.size())\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "torch.Size([10, 10])\n",
      "torch.float64\n",
      "\n",
      "y\n",
      "torch.Size([10, 10])\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "print('x')\n",
    "print(x_new_full.shape)\n",
    "print(x_new_full.dtype)\n",
    "\n",
    "print('\\ny')\n",
    "print(y_new_full.shape)\n",
    "print(y_new_full.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0361, 0.7368, 1.3900],\n",
      "        [1.4168, 0.5115, 1.1214],\n",
      "        [1.3283, 1.0493, 1.7678],\n",
      "        [1.4159, 1.2585, 1.3280],\n",
      "        [1.0942, 0.9379, 0.8717]])\n"
     ]
    }
   ],
   "source": [
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0361, 0.7368, 1.3900],\n",
      "        [1.4168, 0.5115, 1.1214],\n",
      "        [1.3283, 1.0493, 1.7678],\n",
      "        [1.4159, 1.2585, 1.3280],\n",
      "        [1.0942, 0.9379, 0.8717]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0361, 0.7368, 1.3900],\n",
      "        [1.4168, 0.5115, 1.1214],\n",
      "        [1.3283, 1.0493, 1.7678],\n",
      "        [1.4159, 1.2585, 1.3280],\n",
      "        [1.0942, 0.9379, 0.8717]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0361, 0.7368, 1.3900],\n",
       "        [1.4168, 0.5115, 1.1214],\n",
       "        [1.3283, 1.0493, 1.7678],\n",
       "        [1.4159, 1.2585, 1.3280],\n",
       "        [1.0942, 0.9379, 0.8717]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0361, 0.7368, 1.3900],\n",
       "        [1.4168, 0.5115, 1.1214],\n",
       "        [1.3283, 1.0493, 1.7678],\n",
       "        [1.4159, 1.2585, 1.3280],\n",
       "        [1.0942, 0.9379, 0.8717]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.shape, z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(1, 11, size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  9,  1],\n",
      "        [ 1,  8,  1],\n",
      "        [ 4,  3, 10]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x)\n",
    "x[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Bridge\n",
    "---\n",
    "Converting a Torch Tensor to a NumPy array and vice a versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)\n",
    "print(b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting a NumPy Array to Torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Autograd: Automatic Differentiation\n",
    "---\n",
    "[Source](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) of this tutorial\n",
    "\n",
    "PyTorch functions for auto-gradient implementation:\n",
    "1. `torch.tensor`\n",
    "1. `torch.tensor.requires_grad_()`\n",
    "1. `torch.tensor.backward()`\n",
    "1. `tensor.detach()`\n",
    "1. `tensor.grad_fn`\n",
    "\n",
    "The `autograd` package is the cornerstone to all neural networks in `PyTorch`.\n",
    "- `autograd` provides automatic differentiation fot all operations on all tensors. MEMORY\n",
    "- Backpropagation is defined by how your code is excuted.\n",
    "- Every iteration can be different.\n",
    "\n",
    "## Tensor\n",
    "---\n",
    "__`torch.tensor`__ is the fundamental building block in PyTorch.\n",
    "> If `requires_grad=True`, the tensor begins to \"remember\" all operations on it.\n",
    ">\n",
    "> When you call `.backward()` on a `torch.tensor` object that has the attribute `requires_grad=True`, all gradients are computed automatically.\n",
    "    > - The gradient is stored in the `.grad` attribute.\n",
    "\n",
    "To remove a tensor's \"memory\" use `.detach()` to detach it from the computational history. Detach it from its \"experience\". The tensor will also not be able to \"remember\" any future computations performed on it.\n",
    "\n",
    "To prevent a torch from having memory, wrap the code block with:\n",
    "> `with torch.no_grad():`\n",
    "\n",
    "Note: Useful when evaluating a model because the model may have \"trainable parameters\" with `requires_grad=True` and we __don't need the gradients__.\n",
    "\n",
    "`Function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\n",
    "y = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n",
      "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Notice that when we create a new tensor by adding two tensors that have `requires_grad`=True,\n",
    "# The new tensor `z` has memory of how it was created grad_fn=<AddBackward0>.\n",
    "# It knows it was created by addition!\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tensor by mulitplying two existing tensors and a value/scaler\n",
    "a = z * z * 10\n",
    "a_scalar = a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 40., 160., 360.], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(186.6667, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reminder: Tensors can only contain float dtypes.\n",
    "\n",
    "# Similarly, tensor `a` knows that it was created grad_fn=<MulBackward0> == multiplication!\n",
    "print(a)\n",
    "\n",
    "# We'll use a_scalar in the next section gradients\n",
    "print(a_scalar)  # Interesting, this tensor remember the function used on it: grad_fn=<MeanBackward0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(2, 3)\n",
    "b = ((b*10) / (b-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does tensor `b` have autograd enabled? False\n"
     ]
    }
   ],
   "source": [
    "# Reminder: When you create new tensors, you must explicity set requires_grad=True\n",
    "\n",
    "# This tensor does not have autograd enabled. Tensors created from b will not have autograd enabled.\n",
    "print(f\"Does tensor `b` have autograd enabled? {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.8173,  24.6315,   5.1449],\n",
       "        [  1.7535, -77.0677,  -1.3579]], requires_grad=True)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use .requires_grad_() function to add backpropagation to the tensor `b`\n",
    "# Using .requires_grad_(True) modifies the tensor inplace, giving it memory.\n",
    "b.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MulBackward0 object at 0x7f845c6a7d10>\n",
      "None\n",
      "<SumBackward0 object at 0x7f845c6a7d10>\n"
     ]
    }
   ],
   "source": [
    "c = (a * b).sum()\n",
    "print(a.grad_fn)  # created from: z * z * 10\n",
    "print(b.grad_fn)  # A tensor created from scratch will not have memory. Only the ability to memorize.\n",
    "print(c.grad_fn)  # created from: (a * b).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My first backpropagation!\n",
    "a_scalar.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor `x` and tensor `y` are considered leaves, or leaf individually. These two tensors are the origin of `a_scalar`.\n",
    "When backpropagation is executed, the gradient calculations stop at `x` and `y`.\n",
    "\n",
    "`a_scaler` ----Backprop----> `a` ----Backprop----> `z` ----Backprop----> __`x` + `y`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13.3333, 26.6667, 40.0000], dtype=torch.float64)\n",
      "tensor([13.3333, 26.6667, 40.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. Tensor `z` is called a __non-leaf__. Similar to decision tree leafs/pure leafs, this tensor is considered a node. \n",
    "\n",
    "```python\n",
    "print(z.grad)\n",
    "```\n",
    "\n",
    "<div class='alert alert-block alert-danger'>UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
    "This is separate from the ipykernel package so we can avoid doing imports until </div>\n",
    "  \n",
    "### Example of vector-Jacobian product  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  290.8089, -1318.2653,   405.3643], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reminder x2: When creating a new tensor, you must explicitly set requires_grad=True\n",
    "# To perform backprop and give the tensor memory.\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:  # x.data returns the values the tensor object with scalar values\n",
    "    y *= 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "x.data.norm()\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop autograd\n",
    "---\n",
    "- `with torch.no_grad():` code block\n",
    "- `.requires_grad_(False)`\n",
    "- `.detach()`\n",
    "\n",
    "#### `with torch.no_grad():` code block\n",
    "- Wrapping a tensor that was created with requires_grad=True will not be able to pass its __memory abilities__ to new tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does tensor `x` have autograd enabled?\n",
      "True\n",
      "\n",
      "What about a new tensor? Would it have autograd enabled it was created from tensor `x`?\n",
      "True\n",
      "\n",
      "Does tensor `x` have auto_grad enabled now that it's in a torch.no_grad() code block?\n",
      "True\n",
      "\n",
      "What about the new tensor? Would it have auto grad enabled now that it's inside a torch.no_grad(): code block?\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Wrapping a tensor that has requires_grad=True inside a code block\n",
    "# That removes the tensors memory as long as it's in the code block\n",
    "\n",
    "# As we're learning, we know that tensors created from tensors that have requires_grad=True will\n",
    "# pass their memory ability to the new tensor\n",
    "print(f\"Does tensor `x` have autograd enabled?\")\n",
    "print(x.requires_grad)\n",
    "print(\"\\nWhat about a new tensor? Would it have autograd enabled it was created from tensor `x`?\")\n",
    "print((x**2).requires_grad)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"\\nDoes tensor `x` have auto_grad enabled now that it's in a torch.no_grad() code block?\")\n",
    "    print(x.requires_grad)\n",
    "    print(\"\\nWhat about the new tensor? Would it have auto grad enabled now that it's inside a torch.no_grad(): code block?\")\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.requires_grad_(False)`\n",
    "- Most explicit way to remove a tensors' memory is by using `tensor_name.requires_grad_(False)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does tensor `x` have auto_grad enabled? True\n",
      "\n",
      "What about now? False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Does tensor `x` have auto_grad enabled? {x.requires_grad}\")\n",
    "x.requires_grad_(False)\n",
    "print()\n",
    "print(f\"What about now? {x.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.detach`\n",
    "- Use `.detach()` to remove autograd but keep contents of tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does tensor `x` have auto_grad enabled? True\n",
      "Does tensor `y` have auto_grad enabled? False\n",
      "\n",
      "Is tensor `x` equal to tensor `y`?\n",
      "tensor(True)\n",
      "tensor([True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# Set tensor `x` with requires_grad=True to walkthrough using detach()\n",
    "x.requires_grad_(True)\n",
    "\n",
    "print(f\"Does tensor `x` have auto_grad enabled? {x.requires_grad}\")\n",
    "\n",
    "# Create a new tensor from `x` that does not have requires_grad=True. Remove its memory.\n",
    "y = x.detach()\n",
    "\n",
    "print(f\"Does tensor `y` have auto_grad enabled? {y.requires_grad}\", end='\\n\\n')\n",
    "\n",
    "# Although `y` does not have autograd enabled, both tensors contain the same values\n",
    "print(\"Is tensor `x` equal to tensor `y`?\")\n",
    "print(x.eq(y).all())\n",
    "\n",
    "# Another proof of equality\n",
    "print(x==y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 Training a Neural Network by Defining a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
